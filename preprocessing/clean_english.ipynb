{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477dc18e",
   "metadata": {},
   "source": [
    "### Limpiar muestras 1 y 2 en inglés\n",
    "\n",
    "**Nota:** Las muestras en castellano _evaluation1_labeled.csv_ y _vacunes_100rt_evaluation2_label.csv_ corresponden respectivamente a _df_sample_en.csv_ y _df_sample_en2.csv_, que pasa a ser _df_sample_en2-455.csv_ después de corregir algunas cosas usando el código en _translate.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a2cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd, os, numpy as np, csv, re, nltk, string\n",
    "from collections import Counter\n",
    "from time import time\n",
    "from collections import defaultdict \n",
    "from spacy.lang.en import English\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efdced77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('data/df_sample_en.csv', index_col = 0)\n",
    "df_2 = pd.read_csv('data/df_sample_en2-455.csv', index_col = 0)\n",
    "df_sample = pd.concat([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c099bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emojis\n",
    "\n",
    "def extract_emojis(text):\n",
    "    emojis = []\n",
    "    for char in text:\n",
    "        if ord(char) > 600: emojis.append(char)\n",
    "    return emojis\n",
    "\n",
    "emojis_lists = df_sample.content.apply(extract_emojis).tolist()\n",
    "\n",
    "emojis = Counter( [emoji for emojis_list in emojis_lists for emoji in emojis_list] )\n",
    "\n",
    "with open('data/emojis.csv', \"w\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(emojis.most_common())\n",
    "    \n",
    "with open('data/emojis_dict.csv', encoding=\"utf-8\") as csvfile:\n",
    "    emojis = {}\n",
    "    emojis_ranking = []\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i in reader:\n",
    "        if len(i) > 2:\n",
    "            emojis[i[0]] = i[-1]\n",
    "            emojis_ranking.append(i)\n",
    "            \n",
    "\n",
    "slang_data = {}\n",
    "slang_data[':)'] = r'happy'\n",
    "slang_data[':-)'] = r'happy'\n",
    "slang_data[':D'] = r'happy'\n",
    "slang_data[':-D'] = r'happy'\n",
    "\n",
    "slang_data[':*'] = r'kiss'\n",
    "\n",
    "slang_data[\":'(\"] = r'sad'\n",
    "slang_data[':('] = r'sad'\n",
    "slang_data[':-('] = r'sad'\n",
    "slang_data['TT'] = r'sad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18af9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_full_text(text):\n",
    "    hashtags = []\n",
    "    hashtags_ = re.findall(r'#[A-ZÀ-ßa-zà-ý]+', text)\n",
    "    mention_ = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "\n",
    "    for hashtag in hashtags_:\n",
    "        text = re.sub(hashtag, '', text)\n",
    "        expanded = \" \".join([re.sub(r'[#\\s]', '', token) for token in re.split(r'([A-ZÀ-ß][a-zà-ý]*)', hashtag) if token and not re.match(r'^\\s$', hashtag)]).strip()\n",
    "        expanded = re.sub(r'\\b(\\S)\\s+(?=\\S\\b)', r'\\1', expanded)\n",
    "        hashtags.append(expanded)\n",
    "\n",
    "    text = re.sub(r'\\s', ' ', text + '. '.join(hashtags))\n",
    "    text = re.sub(mention_, \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c8b637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['preprocessed_text'] = df_sample.content.apply(clean_full_text)\n",
    "for key, value in emojis.items(): df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(key,' ' +  value + ' ')# 'emoji ')\n",
    "\n",
    "\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.lower()\n",
    "df_sample['slang_data'] = 0\n",
    "for key, value in slang_data.items():\n",
    "    df_sample['slang_data'] += df_sample['preprocessed_text'].str.count(pat=re.escape(key))\n",
    "    df_sample['preprocessed_text'] = df_sample.preprocessed_text.str.replace(pat=re.escape(key), repl=' ' + value + ' ', regex=True)\n",
    "    \n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat=r\"\\d\", repl=' ', regex=True) #remove numbers\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat='[' + re.escape('?¿') + ']', repl=' ', regex=True)\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat='[' + re.escape('!¡') + ']', repl=' ', regex=True)\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat='[' + re.escape('?¿') + ']', repl=' ', regex=True)\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat='[' + re.escape('!¡') + ']', repl=' ', regex=True)\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat='[' + re.escape(string.punctuation) + r'\\n\\t]', repl=' ', regex=True)\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.strip()\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].str.replace(pat=r'\\s+', repl=' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fec39da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.29 mins\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "t = time()\n",
    "\n",
    "# Lemmatize and remove stop words\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].apply(lambda x: ' '.join([l.lemma_ for l in nlp(x) if l.text not in stopwords]))\n",
    "\n",
    "df_sample['punctuation'] = df_sample['content'].str.count(pat='[' + re.escape(string.punctuation) + r'!¿?¡]')\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a32c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sentiment\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment\n",
    "\n",
    "sentiment = df_sample['preprocessed_text'].apply(get_polarity)\n",
    "\n",
    "\n",
    "df_sample['polarity'] = sentiment.apply(lambda x: x[0])\n",
    "df_sample['subjectivity'] = sentiment.apply(lambda x: x[1])\n",
    "\n",
    "\n",
    "# Classes based on polarity scores\n",
    "df_sample['pol_labels'] = [1 if x > 0 else 2 if x == 0 else 0 for x in df_sample['polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8520a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample.to_csv('data/all_sample.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
